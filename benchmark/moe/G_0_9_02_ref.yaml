test_name: G_0_9_01
global: |
  E_value = 3
  ctx[E] = E_value
  softmax = torch.nn.Softmax(dim=-1)
  gelu = torch.nn.GELU()

inputs:
  - name: E0
    dtype: fp32
    dims: [K, N, M]
    data_gen: torch.randn

parameters:
  - name: Wg
    dtype: fp32
    dims: [E, K]
    data_gen: torch.randn
  - name: W0_0
    dtype: fp32
    dims: [D, K]
    data_gen: torch.randn
  - name: W0_1
    dtype: fp32
    dims: [K, D]
    data_gen: torch.randn
  - name: W1_0
    dtype: fp32
    dims: [D, K]
    data_gen: torch.randn
  - name: W1_1
    dtype: fp32
    dims: [K, D]
    data_gen: torch.randn
  - name: W2_0
    dtype: fp32
    dims: [D, K]
    data_gen: torch.randn
  - name: W2_1
    dtype: fp32
    dims: [K, D]
    data_gen: torch.randn

fns:
  - name: Gate
    apply: |
      return [softmax(input[0] @ input_data['Wg'])]
    input_dtype: Buffer(fp32, [K])
    output_dtype: Buffer(fp32, [E])
    func_name: fn_gate

  - name: Top2
    apply: |
      _, indices = torch.topk(input[0], 2, dim=-1)
      multihot = torch.zeros_like(input[0], dtype=torch.float32)
      multihot.scatter_(-1, indices, 1.0)
      return [multihot]
    input_dtype: Buffer(fp32, [E])
    output_dtype: Multihot(fp32, E)
    func_name: fn_top2

  - name: Sum
    apply: |
      return [state[0] + input[0]]
    init: [0]
    input_dtype: Buffer(fp32, [K])
    output_dtype: Buffer(fp32, [K])
    func_name: fn_sum
  
  - name: Expert0
    apply: |
      return [gelu(input[0] @ input_data['W0_0']) @ input_data['W0_1'] * input[1][0]]
    input_dtype: ["Buffer(fp32, [K])", "Buffer(fp32, [E])"]
    output_dtype: Buffer(fp32, [K])
    func_name: fn_expert0
  
  - name: Expert1
    apply: |
      return [gelu(input[0] @ input_data['W1_0']) @ input_data['W1_1'] * input[1][1]]
    input_dtype: ["Buffer(fp32, [K])", "Buffer(fp32, [E])"]
    output_dtype: Buffer(fp32, [K])
    func_name: fn_expert1
  
  - name: Expert2
    apply: |
      return [gelu(input[0] @ input_data['W2_0']) @ input_data['W2_1'] * input[1][2]]
    input_dtype: ["Buffer(fp32, [K])", "Buffer(fp32, [E])"]
    output_dtype: Buffer(fp32, [K])
    func_name: fn_expert2

outputs:
  - name: S0
    dtype: Buffer(fp32, [K])
    dims: [N, M]
    data_transform:
      - |
        affinity = softmax(input_data['E0'] @ input_data['Wg'])
        _, indices = torch.topk(affinity, 2, dim=-1)
        multihot = torch.zeros_like(affinity, dtype=torch.float32)
        index_tensor = multihot.scatter(-1, indices, 1.0).movedim(-1, 0)
        W0 = torch.stack([input_data[f'W{i}_0'] for i in range(E_value)])
        W1 = torch.stack([input_data[f'W{i}_1'] for i in range(E_value)])
        hidden = torch.einsum('mnk,ekh->emnh', input_data['E0'], W0)
        hidden = gelu(hidden)
        transformed = torch.einsum('emnh,ehk->emnk', hidden, W1)
        weighted = transformed * affinity.movedim(-1, 0).unsqueeze(-1)
        (index_tensor.unsqueeze(-1) * weighted).sum(dim=0)

impl: |
  expert_fns = [fn_expert0, fn_expert1, fn_expert2]
  E1 = step.Bufferize(a=1).apply(E0)
  E2 = step.Map(fn=fn_gate).apply(E1)
  E3 = step.Zip().apply((E1, E2))
  E4 = step.Map(fn=fn_top2).apply(E2)
  E5, E6 = step.Copy().apply(E4)
  E7 = step.Partition(N=E_value).apply((E3, E5))
  E8 = [step.Map(fn=f).apply(s) for s, f in zip(E7, expert_fns)]
  E9 = step.Merge(fn=fn_sum).apply((E8, E6))
  return E9